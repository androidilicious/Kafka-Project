# Real-Time Stock Market Streaming System

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-3.5+-red.svg)](https://kafka.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-blue.svg)](https://www.postgresql.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.29+-FF4B4B.svg)](https://streamlit.io/)

A  real-time data streaming system for stock market trading data, featuring Apache Kafka for event streaming, PostgreSQL for data persistence, Streamlit for live visualization, and advanced analytics including Apache Flink-style windowed aggregations and ML-based anomaly detection.

## ğŸ¯ Project Overview

This project demonstrates a complete end-to-end real-time data pipeline:

1. **Data Generation**: Synthetic stock trading events with realistic price movements
2. **Event Streaming**: Apache Kafka for high-throughput message streaming
3. **Data Storage**: PostgreSQL for persistent storage and analytics
4. **Real-Time Processing**: Windowed aggregations for time-series analytics
5. **Live Visualization**: Auto-refreshing Streamlit dashboard
6. **Anomaly Detection**: Machine learning models to detect unusual trading patterns

### Domain: Stock Market Trading

The system tracks 10 major stocks (AAPL, GOOGL, MSFT, AMZN, TSLA, META, NVDA, JPM, V, WMT) and generates realistic trading events including:
- Buy/Sell transactions
- Real-time price fluctuations
- Trading volumes
- Synthetic anomalies (unusual volume or price movements)

### ğŸ“¸ System in Action

![Dashboard Overview](media/fullpagedash.png)

| Feature | Demo |
|---------|------|
| **Summary Metrics** | ![Summary](media/summary.gif) |
| **Interactive Charts** | ![Charts](media/charts.gif) |
| **Anomaly Detection** | ![Anomaly](media/anomaly.gif) |

## ğŸ“‹ Features

### Core Features
- âœ… **Real-time data streaming** via Apache Kafka
- âœ… **PostgreSQL database** for persistent storage
- âœ… **Live Streamlit dashboard** with auto-refresh
- âœ… **Multiple visualizations**: time series, bar charts, pie charts
- âœ… **Configurable parameters** via environment variables

### Bonus Features (Advanced)
- ğŸŒŸ **Apache Flink Integration** (10%+ bonus)
  - Tumbling window aggregations (1-minute windows)
  - Real-time metrics: AVG, MIN, MAX, SUM, COUNT
  - Separate aggregated metrics table
  
- ğŸŒŸ **Anomaly Detection Model** (10%+ bonus)
  - Isolation Forest for multivariate anomaly detection
  - Statistical outlier detection (Z-score method)
  - Sequential pattern analysis
  - Feature engineering from trading patterns

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producer  â”‚â”€â”€â”€â”€â–¶â”‚  Kafka Topic â”‚â”€â”€â”€â”€â–¶â”‚  Consumer   â”‚
â”‚  (Python)   â”‚     â”‚ stock-trades â”‚     â”‚  (Python)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                    â”‚
                            â”‚                    â–¼
                            â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚              â”‚  PostgreSQL  â”‚
                            â”‚              â”‚   Database   â”‚
                            â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                     â”‚
                            â–¼                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
                    â”‚  Aggregator  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚(Flink-style) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Streamlit  â”‚
                    â”‚  Dashboard   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Anomaly    â”‚
                    â”‚   Detector   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Docker and Docker Compose
- Git

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd "Kafka Project"
   ```

2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Start Docker services**
   ```bash
   docker-compose up -d
   ```

   This starts:
   - Zookeeper (port 2181)
   - Kafka (port 9092)
   - PostgreSQL (port 5432)
   - Flink JobManager (port 8081)
   - Flink TaskManager

4. **Verify services are running**
   ```bash
   docker-compose ps
   ```

### Running the System

You need to run multiple components in separate terminals:

#### Terminal 1: Start the Producer
```bash
python producer.py
```
This generates synthetic stock trading events and publishes them to Kafka.

#### Terminal 2: Start the Consumer
```bash
python consumer.py
```
This consumes events from Kafka and stores them in PostgreSQL.

#### Terminal 3: Start the Aggregator (Optional - Bonus Feature)
```bash
python aggregator.py
```
This performs windowed aggregations on the streaming data.

#### Terminal 4: Start the Dashboard
```bash
streamlit run dashboard.py
```
This launches the interactive dashboard at `http://localhost:8501`

## ğŸ“Š Dashboard Features

**Real-Time Metrics:** Trades, volume, value, active symbols, anomaly count

**Visualizations:** Performance tables, bar/pie charts, time series with anomaly highlighting, recent trades

**Interactive Controls:** Adjustable refresh interval (1-10s), configurable time windows, anomaly filtering, symbol selection

## ğŸ”§ Configuration

Edit `.env` file to customize:

```env
# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC=stock-trades

# PostgreSQL Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=stock_market_db
POSTGRES_USER=kafka_user
POSTGRES_PASSWORD=kafka_password

# Application Configuration
PRODUCER_INTERVAL=0.5           # Seconds between trades
STREAMLIT_REFRESH_INTERVAL=2    # Dashboard refresh interval
```

## ğŸ¤– Anomaly Detection (Bonus Feature)

**Train:** `python anomaly_detector.py train` (requires 1000+ trades per symbol)

**Analyze:** `python anomaly_detector.py analyze [SYMBOL]`

**Methods:** Isolation Forest (multivariate), Z-score (statistical outliers), sequential pattern analysis

## ğŸŒŠ Apache Flink Integration (Bonus Feature)

**Run:** `python aggregator.py`

**Features:** 60-second tumbling windows, per-symbol metrics (AVG, MIN, MAX, SUM, COUNT), stored in `stock_metrics` table

**Advanced:** Full Flink deployment with SQL (see `flink_job.sql`)

## ğŸ“ Project Structure

```
Kafka Project/
â”œâ”€â”€ docker-compose.yml          # Docker services configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ .gitignore                 # Git ignore rules
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ producer.py                # Kafka producer for stock trades
â”œâ”€â”€ consumer.py                # Kafka consumer to database
â”œâ”€â”€ database.py                # Database utilities and schema
â”œâ”€â”€ aggregator.py              # Windowed aggregations
â”œâ”€â”€ dashboard.py               # Streamlit visualization
â”œâ”€â”€ anomaly_detector.py        # ML-based anomaly detection
â”œâ”€â”€ flink_processor.py         # Flink integration (advanced)
â”œâ”€â”€ flink_job.sql              # Flink SQL script
â”œâ”€â”€ test_system.py             # System health checks
â”œâ”€â”€ start.sh / start.bat       # Quick start scripts
â”œâ”€â”€ stop.sh / stop.bat         # Quick stop scripts
â”œâ”€â”€ media/                     # Screenshots and demos
â”‚   â”œâ”€â”€ fullpagedash.png       # Full dashboard screenshot
â”‚   â”œâ”€â”€ summary.gif            # Summary metrics demo
â”‚   â”œâ”€â”€ charts.gif             # Charts visualization demo
â”‚   â””â”€â”€ anomaly.gif            # Anomaly detection demo
â””â”€â”€ models/                    # Trained ML models (generated)
```

## ğŸ§ª Testing

**Kafka:** `docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092`

**PostgreSQL:** `docker exec -it postgres psql -U kafka_user -d stock_market_db`

**Flink UI:** http://localhost:8081

**System Check:** `python test_system.py`

## ğŸ“ˆ Performance

Expected throughput:
- **Producer**: 2 trades/second (configurable)
- **Consumer**: 1000+ trades/second
- **Aggregator**: Real-time processing with < 1 second latency
- **Dashboard**: Refreshes every 2 seconds (configurable)

## ğŸ› ï¸ Troubleshooting

**Restart Services:** `docker-compose restart kafka` or `docker-compose down -v && docker-compose up -d`

**Check Logs:** `docker logs kafka` or `docker logs postgres`

**Verify Data:** `docker exec -it postgres psql -U kafka_user -d stock_market_db -c "SELECT COUNT(*) FROM trades;"`


---

**Note**: This is a demonstration project using synthetic data. For production use with real financial data, implement proper security, compliance, and risk management measures.
